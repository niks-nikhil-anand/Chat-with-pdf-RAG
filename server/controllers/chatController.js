import { QdrantVectorStore } from "@langchain/qdrant";
import {
  GoogleGenerativeAIEmbeddings,
  ChatGoogleGenerativeAI,
} from "@langchain/google-genai";
import dotenv from "dotenv";

dotenv.config();

export const chatController = async (req, res) => {
  try {
    const { query } = req.body;
    console.log("ğŸ“© Incoming request with query:", query);

    if (!query || typeof query !== "string") {
      console.warn("âš ï¸ Invalid query received:", query);
      return res
        .status(400)
        .json({ error: "Query is required and must be a string" });
    }

    // âœ… Create embeddings
    console.log("ğŸ”¹ Initializing embeddings model...");
    const embeddings = new GoogleGenerativeAIEmbeddings({
      model: "models/embedding-001",
      apiKey: process.env.GOOGLE_API_KEY,
    });

    // âœ… Connect to Qdrant
    console.log("ğŸ”¹ Connecting to Qdrant at:", process.env.QDRANT_URL);
    const vectorStore = await QdrantVectorStore.fromExistingCollection(
      embeddings,
      {
        url: process.env.QDRANT_URL,
        collectionName: "pdf_embeddings",
      }
    );

    const retriever = vectorStore.asRetriever({ k: 2 });
    console.log("ğŸ”¹ Retriever created with top-k = 2");

    // âœ… Retrieve similar docs
    console.log("ğŸ”¹ Performing similarity search...");
    const similaritySearchResults = await retriever.invoke(query);
    console.log(
      "ğŸ“„ Similarity Search Results:",
      JSON.stringify(similaritySearchResults, null, 2)
    );

    // Build system + context prompt
    const SYSTEM_PROMPT = `
      You are a helpful AI assistant. 
      Use the following context from a PDF to answer the userâ€™s question and answer in 50 words atleast.
      
      Context: ${JSON.stringify(similaritySearchResults, null, 2)}
      
      User Question: ${query}
    `;
    console.log("ğŸ“ Built system prompt.");

    // âœ… Call Gemini LLM
    console.log("ğŸ¤– Calling Gemini model...");
    const llm = new ChatGoogleGenerativeAI({
      apiKey: process.env.GOOGLE_API_KEY,
      model: "gemini-1.5-flash",
      temperature: 0.7,
    });

    const data = await llm.invoke(SYSTEM_PROMPT);
    console.log("âœ… Gemini response received.");

    const aiContent = data;
    console.log("AI Content:", aiContent);

    return res.json({
      aiContent,
      similaritySearchResults,
    });
  } catch (error) {
    console.error("âŒ Error in /chat:", error);
    res.status(500).json({ error: error.message });
  }
};
